{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Maryland Inverse Design Benchmark Suite The Maryland Inverse Design (MID) Benchmark Suite is a set of libraries for running computational experiments on Machine Learning models for Inverse Design across a wide variety of domains and metrics. Its goal is to facilitate reproducible research in this area and help broaden the applicability of various ID algorithms across many applications by making it easy to run otherwise complex engineering design simulations using a common interface specification. It also provides a set of metrics that are relevant to different aspects of ID performance commonly used in research papers. In this way, if a researcher designs a new kind of ID algorithm, they will be able to use this repository to run a large set of tests across common examples of increasing range and complexity. You can read more about the library at its documentation website . Contents Specifically, this repository contains the following: - A set of Engineering Design Environments (see env folder) that provides containerized simulation environments in which researchers can run their ID algorithm across a variety of multi-physics simulation and optimization platforms. These will implement a range of applications from Aerodynamic Optimization to Power Electronics to Medical Devices to Photonics to Heat Transfer, among others as we develop and deploy those environments. Currently available environments are in the env folder. - Code that links to/downloads a set of pre-built datasets generated from the above environments that you can use for training your Inverse Design algorithm. These are helpful if you just wish to train an Inverse Design model directly from an existing dataset without needing to run any simulation or design optimization code (which is typical time consuming). While you will not be able to calculate certain ID metrics without using the containerized environments mentioned above, there are certain metrics that you can compute using only the dataset, making this is a useful and lower-cost starting point for many researchers. - A set of Python files for computing a common set of ID performance metrics ( metrics.py ). These include simpler metrics like those that measure closeness of distribution to test data, along with more complex metrics that require access to a design environment or simulator, such as Instantaneous Optimality Gap and Cumulative Optimality Gap which is useful for judging how well ID warm starts existing optimization routines. Installation pip install midbench However, to use specific design environments will require installing either Singularity or Docker and using our pre-build images defined within that environment's singularity container file ( i.e. , the .sif file). These are described further in the documentation pages. Certain environments may require some additional installation steps -- we are working to simplify these, but in the meantime each environments subpage will provide some more details on environment specific installation. Usage Using the benchmark suite has essentially three steps, some of which may be optional depending on how you prefer to use the library: 1. (Optional) Load an existing ID dataset and train an ID model on that dataset. Alternatively, you can load a pre-trained model. 2. Specify a set of input conditions over which you wish to generate predictions/designs and then generate sample designs for those inputs. These typically include requirements for the design that the ID outputs should satisfy. 3. Analyze how well the generated designs do us with respect to various ID metrics, such as MMD, Instantaneous Optimality Gap and Cumulative Optimality Gap. An example usage case is provided in the tutorials\\ folder.","title":"Overview"},{"location":"#the-maryland-inverse-design-benchmark-suite","text":"The Maryland Inverse Design (MID) Benchmark Suite is a set of libraries for running computational experiments on Machine Learning models for Inverse Design across a wide variety of domains and metrics. Its goal is to facilitate reproducible research in this area and help broaden the applicability of various ID algorithms across many applications by making it easy to run otherwise complex engineering design simulations using a common interface specification. It also provides a set of metrics that are relevant to different aspects of ID performance commonly used in research papers. In this way, if a researcher designs a new kind of ID algorithm, they will be able to use this repository to run a large set of tests across common examples of increasing range and complexity. You can read more about the library at its documentation website .","title":"The Maryland Inverse Design Benchmark Suite"},{"location":"#contents","text":"Specifically, this repository contains the following: - A set of Engineering Design Environments (see env folder) that provides containerized simulation environments in which researchers can run their ID algorithm across a variety of multi-physics simulation and optimization platforms. These will implement a range of applications from Aerodynamic Optimization to Power Electronics to Medical Devices to Photonics to Heat Transfer, among others as we develop and deploy those environments. Currently available environments are in the env folder. - Code that links to/downloads a set of pre-built datasets generated from the above environments that you can use for training your Inverse Design algorithm. These are helpful if you just wish to train an Inverse Design model directly from an existing dataset without needing to run any simulation or design optimization code (which is typical time consuming). While you will not be able to calculate certain ID metrics without using the containerized environments mentioned above, there are certain metrics that you can compute using only the dataset, making this is a useful and lower-cost starting point for many researchers. - A set of Python files for computing a common set of ID performance metrics ( metrics.py ). These include simpler metrics like those that measure closeness of distribution to test data, along with more complex metrics that require access to a design environment or simulator, such as Instantaneous Optimality Gap and Cumulative Optimality Gap which is useful for judging how well ID warm starts existing optimization routines.","title":"Contents"},{"location":"#installation","text":"pip install midbench However, to use specific design environments will require installing either Singularity or Docker and using our pre-build images defined within that environment's singularity container file ( i.e. , the .sif file). These are described further in the documentation pages. Certain environments may require some additional installation steps -- we are working to simplify these, but in the meantime each environments subpage will provide some more details on environment specific installation.","title":"Installation"},{"location":"#usage","text":"Using the benchmark suite has essentially three steps, some of which may be optional depending on how you prefer to use the library: 1. (Optional) Load an existing ID dataset and train an ID model on that dataset. Alternatively, you can load a pre-trained model. 2. Specify a set of input conditions over which you wish to generate predictions/designs and then generate sample designs for those inputs. These typically include requirements for the design that the ID outputs should satisfy. 3. Analyze how well the generated designs do us with respect to various ID metrics, such as MMD, Instantaneous Optimality Gap and Cumulative Optimality Gap. An example usage case is provided in the tutorials\\ folder.","title":"Usage"},{"location":"baselines/","text":"MID Baseline Models This page provides links and descriptions to various existing baseline ID models that we have integrated into the MID Benchmark Suite.","title":"Baselines"},{"location":"baselines/#mid-baseline-models","text":"This page provides links and descriptions to various existing baseline ID models that we have integrated into the MID Benchmark Suite.","title":"MID Baseline Models"},{"location":"contributing-environments/","text":"Contributing New Simulation Environments or Datasets While we designed the MID Benchmark Suite to contain a range of possible simulation baselines relevant to Inverse Design across a range of fields, we recognize that the provided environments are non-exhaustive. Moreover, as the solvers upon which our current environments improve, we recognize that others may wish to contribute updates to those environments. We are working on providing documentation for how to independently contribute your own simulation environments to add to the MID Benchmark Suite, but for now if you are interested in helping with this, reach out to the development team and we can provide further details.","title":"Contributing New Environments"},{"location":"contributing-environments/#contributing-new-simulation-environments-or-datasets","text":"While we designed the MID Benchmark Suite to contain a range of possible simulation baselines relevant to Inverse Design across a range of fields, we recognize that the provided environments are non-exhaustive. Moreover, as the solvers upon which our current environments improve, we recognize that others may wish to contribute updates to those environments. We are working on providing documentation for how to independently contribute your own simulation environments to add to the MID Benchmark Suite, but for now if you are interested in helping with this, reach out to the development team and we can provide further details.","title":"Contributing New Simulation Environments or Datasets"},{"location":"datasets/","text":"MID Benchmark Suite -- Datasets One major component of the Maryland Inverse Design Benchmark Suite is the availability of datasets needed to train the various Inverse Design models. This page describes our general approach for how we construct these datasets and provides links to the various environments where datasets are located. In general, each environment will possess one or more datasets that was generated specific to that environment, though it is certainly possible to use datasets generated for one environment one another environment or ID task. Dataset Management The $(\\bm{d}, \\bm{b}, \\bm{p})$---design, condition, performance---tuples generated by the environments are handled by the following two classes--- DataEntry and DataBase . DataBase can be used to produce the required dataset object for inverse design learning. DataEntry class midbench.data.DataEntry(designs: Design, conditions: Condition, performances: Performance, specs: Union[SimSpec, OptSpec, str, NoneType]=None) This is an iterable object for storing and processing the $(\\bm{d,b,p})$ tuple that user received during a single simulation or optimization process. It is the output of the Env.simulate() and Env.optimize() method. It is built for easy indexing and slicing across all the three elements. It can be subclassed to get environment specific DataBase. Parameters designs (Design): The designs. conditions (Condition): The boundary conditions. performances (Performance): The corresponding performances. specs (Union[SimSpec, OptSpec, str, NoneType]): The specifications of the operation by which this data is obtained. This is for experiment replicability. Can also be filled with text or just left empty. Methods _is_compatible(designs: Design, conditions: Condition, performances: Performance) \u2192 Bool This checks if the size of the first dimension of the designs, conditions and performances are compatible when a new DataEntry instance is to be created. - Paramaters - designs (Design): The designs. - conditions (Condition): The boundary conditions. - performances (Performance): The corresponding performances. - Returns - True or False - convert(design_specs: DesignSpec, c_units: Union[list, tuple, str], p_units: Union[list, tuple, str]) \u2192 DataEntry This is for generating a new data entry of the given formats. - Paramaters - design_specs (DesignSpec): The specifications of the design space to which the current design variables are to be converted. - c_units (Union[list, tuple, str]): The boundary condition units sent to the convert method. - p_units (Union[list, tuple, str]): The performance units sent to the convert method. - Returns - A new DataEntry instance of the given formats. - ...maybe some statistics and plotting methods. - __getitem__(idx) \u2192 DataEntry - __iter__() - __next__() \u2192 DataEntry - Attributes - designs (Design) - conditions (Conditions) - performances (Performances) DataBase class midbench.data.DataBase(*entries: DataEntry) Database is a list-like mutable object for managing all the data entries that users collected over time, and producing unified dataset for inverse design training tasks. It should also provide necessary methods for curating, e.g., ones that can remove duplicate entries or produce dataset of certain diversity. It can be subclassed to get environment specific DataBase. Methods generate_dataset(design_specs: DesignSpec, c_units: Union[list, tuple, str], p_units: Union[list, tuple, str], p_query: Tensor) \u2192 Dataset: Generate the PyTorch dataset tensor of given format and size for inverse design training. Parameters design_specs (DesignSpec): The specifications of the design space to which the current design variables are to be converted. c_units (Union[list, tuple, str]): The boundary condition units sent to the convert method. p_units (Union[list, tuple, str]): The performance units sent to the convert method. p_query (Tensor): Only data entries of matched p_query can be selected to compose new dataset, as we don't need NaN. Returns A Pytorch or TensorFlow Dataset instance. append(entry: DataEntry) \u2192 None: Parameters: entry (DataEntry) - The data entry to be merged into the database. diversify() view(class) Dataset Compatibility Even if DataEntry and DataBase might be environment specific through subclassing, different pairs of these two classes may still be compatible with each other as long as they have the same parent class. After all, what really matters here is the value of the dbp tuple. We provide a view() method similar to the one in NumPy to enable easy migration between different DataEntry and DataBase classes. Available Dataset SU2 Airfoil 2D","title":"Datasets"},{"location":"datasets/#mid-benchmark-suite-datasets","text":"One major component of the Maryland Inverse Design Benchmark Suite is the availability of datasets needed to train the various Inverse Design models. This page describes our general approach for how we construct these datasets and provides links to the various environments where datasets are located. In general, each environment will possess one or more datasets that was generated specific to that environment, though it is certainly possible to use datasets generated for one environment one another environment or ID task.","title":"MID Benchmark Suite -- Datasets"},{"location":"datasets/#dataset-management","text":"The $(\\bm{d}, \\bm{b}, \\bm{p})$---design, condition, performance---tuples generated by the environments are handled by the following two classes--- DataEntry and DataBase . DataBase can be used to produce the required dataset object for inverse design learning.","title":"Dataset Management"},{"location":"datasets/#dataentry","text":"class midbench.data.DataEntry(designs: Design, conditions: Condition, performances: Performance, specs: Union[SimSpec, OptSpec, str, NoneType]=None) This is an iterable object for storing and processing the $(\\bm{d,b,p})$ tuple that user received during a single simulation or optimization process. It is the output of the Env.simulate() and Env.optimize() method. It is built for easy indexing and slicing across all the three elements. It can be subclassed to get environment specific DataBase. Parameters designs (Design): The designs. conditions (Condition): The boundary conditions. performances (Performance): The corresponding performances. specs (Union[SimSpec, OptSpec, str, NoneType]): The specifications of the operation by which this data is obtained. This is for experiment replicability. Can also be filled with text or just left empty. Methods _is_compatible(designs: Design, conditions: Condition, performances: Performance) \u2192 Bool This checks if the size of the first dimension of the designs, conditions and performances are compatible when a new DataEntry instance is to be created. - Paramaters - designs (Design): The designs. - conditions (Condition): The boundary conditions. - performances (Performance): The corresponding performances. - Returns - True or False - convert(design_specs: DesignSpec, c_units: Union[list, tuple, str], p_units: Union[list, tuple, str]) \u2192 DataEntry This is for generating a new data entry of the given formats. - Paramaters - design_specs (DesignSpec): The specifications of the design space to which the current design variables are to be converted. - c_units (Union[list, tuple, str]): The boundary condition units sent to the convert method. - p_units (Union[list, tuple, str]): The performance units sent to the convert method. - Returns - A new DataEntry instance of the given formats. - ...maybe some statistics and plotting methods. - __getitem__(idx) \u2192 DataEntry - __iter__() - __next__() \u2192 DataEntry - Attributes - designs (Design) - conditions (Conditions) - performances (Performances)","title":"DataEntry"},{"location":"datasets/#database","text":"class midbench.data.DataBase(*entries: DataEntry) Database is a list-like mutable object for managing all the data entries that users collected over time, and producing unified dataset for inverse design training tasks. It should also provide necessary methods for curating, e.g., ones that can remove duplicate entries or produce dataset of certain diversity. It can be subclassed to get environment specific DataBase. Methods generate_dataset(design_specs: DesignSpec, c_units: Union[list, tuple, str], p_units: Union[list, tuple, str], p_query: Tensor) \u2192 Dataset: Generate the PyTorch dataset tensor of given format and size for inverse design training. Parameters design_specs (DesignSpec): The specifications of the design space to which the current design variables are to be converted. c_units (Union[list, tuple, str]): The boundary condition units sent to the convert method. p_units (Union[list, tuple, str]): The performance units sent to the convert method. p_query (Tensor): Only data entries of matched p_query can be selected to compose new dataset, as we don't need NaN. Returns A Pytorch or TensorFlow Dataset instance. append(entry: DataEntry) \u2192 None: Parameters: entry (DataEntry) - The data entry to be merged into the database. diversify() view(class)","title":"DataBase"},{"location":"datasets/#dataset-compatibility","text":"Even if DataEntry and DataBase might be environment specific through subclassing, different pairs of these two classes may still be compatible with each other as long as they have the same parent class. After all, what really matters here is the value of the dbp tuple. We provide a view() method similar to the one in NumPy to enable easy migration between different DataEntry and DataBase classes.","title":"Dataset Compatibility"},{"location":"datasets/#available-dataset","text":"SU2 Airfoil 2D","title":"Available Dataset"},{"location":"environments/","text":"MID Benchmark Suite -- Environments The environments in MIDBench serve as the interface between Python and the simulation & optimization (S&O) programs---be it a script running locally, or one running inside a Docker or Singularity container. Specifically, it should either perform simulation by taking the design variables and boundary conditions as input then yielding the performance metrics as requested, or perform optimization over the initial design variables by optimizing the performance metrics under the given boundary conditions. Since each S&O program is not necessarily written in Python, the design variables and boundary conditions need to be first transformed into a format readable to the program, and the resulting output of the program needs to be transformed back into Python variables. In short, the MIDBench Environments are for generating the $(\\bm{d}, \\bm{b}, \\bm{p})$---design, condition, performance---tuples for inverse design learning. Basic Components The MIDBench environments should complete their jobs with the following environment-specific Python classes--- Env , Design , Condition , Performance ---as components for easy management: Env class midbench.core.Env(specs: EnvSpec = None) The parent class of the simulation/optimization environment, which serves as the interface between Python and the simulation/optimization program. In other words, it takes Python variables as inputs and outputs the results also in form of Python variables, while keeping the processing and running of the program under the hood. Parameters: specs ( EnvSpec ): The specifications for the initialization of the S&O program. Methods As its role suggests, Env should have either a \"simulate\" method, or a \"optimize\" method. Both are for generating the $(\\bm{d}, \\bm{b}, \\bm{p})$ tuples for inverse design learning, but with different motivations. simulate(designs: Design, conditions: Condition, performances: Performance, specs: SimSpec) \u2192 DataEntry: Motivation: Design & Condition \u2192 Performance - Parameters - designs ( Design ) - An $N\\times D$ tensor-liked object defining the design space and storing the set of design variables $\\bm{d}$. - conditions ( Condition ) - An $N\\times B$ or $1\\times B$ tensor-liked object storing the set of boundary conditions $\\bm{b}$. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - performances ( Performance ) - An $N\\times P$ or $1\\times P$ tensor-liked object storing the set of performances. It should be empty, with only the ID of the performances to be evaluated specified. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - specs ( SimSpec ) - The specifications for running the simulation. Should be a Dict -like object. - Returns - The DataEntry data logger storing the $(\\bm{d}, \\bm{b}, \\bm{p})$ tuples, with the SimSpec instance included for simulation replicability. - optimize(designs: Design, conditions: Condition, performances: Performance, specs: OptSpec) \u2192 DataEntry: Motivation: Performance & Condition \u2192 Design - Parameters - designs ( Design ) - An $N\\times D$ tensor-liked object defining the design space and storing the set of initial design variables $\\bm{d}$. - conditions ( Condition ) - An $N\\times B$ or $1\\times B$ tensor-liked object storing the set of boundary conditions $\\bm{b}$ under which the optimization is performed. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - performances ( Performance ) - An $N\\times P$ or $1\\times P$ tensor-liked object storing the set of performances to be optimized. It should be empty, with only the ID of the performances to be evaluated specified. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - specs ( OptSpec ): The specifications for running the optimization, including things like the number of iterations, tolerances, etc. Should be a Dict -like object. - Returns - The DataEntry data entry storing the $(\\bm{d}^\\star, \\bm{b}, \\bm{p}^\\star)$ tuples for either the optimization final result or the entire optimization history, with the OptSpec instance included for optimization replicability. Attributes: dbp (tuple): The tuple (Design, Condition, Performance) of classes dedicated to handling the data for this specific environment. Design class midbench.core.Design(data: array_like, specs: DesignSpec=None, dtype=None, **kwargs) An ndarray subclass serving as the container and manager for the design variables $\\bm{d}$ of a list of designs. We can develop environment-specific Designs by inheriting and overloading. Arguments data ( array_like ): The tensor of design variables or the directory of the design files. specs ( DesignSpec ): A dict -like object containing the specifications of the current design variable space. For instance, the specifications of the control point spaces of Bezier curve or B-spline. dtype ( data-type, optional ): dtype for ndarray. kwargs: Optional keyword arguments used as key-value pairs to update the specifications in the specs attribute. Cannot be any key other than the existing keys in specs . Methods convert(specs: DesignSpec) \u2192 Design: Convert the current design variables into a different design variable space. Parameters specs ( DesignSpec ): The specification of the target design variable space. Returns A new Design in the target design space. output(specs: MeshSpec) \u2192 Mesh: Generate mesh file readable to the S&O program. Parameters specs ( MeshSpec ): The specifications of the mesh. Returns A mesh file (or its directory) that is readable to the environment. Attributes specs ( DesignSpec ): The specifications of the current design space. Env ( Type ): The Env class that this Design class is built for. Spec( Type ): The DesignSpec class dedicated to this Design class. A mandatory attribute in the definition of Design subclass. Condition class midbench.core.Condition(data: array_like, specs: ConditionSpec=None, dtype=None, **kwargs) The container and manager for boundary conditions. This is the parent class of every other env-specific Condition. Arguments data ( array_like ): The values of the boundary conditions. specs ( ConditionSpec ): An OrderedDict -like object containing the names and units of the values, and also specifying their order. Can be ignored if we only use the default metric system. dtype ( data-type, optional ): dtype for ndarray. kwargs: Optional keyword arguments used as key-value pairs to update the specifications in the specs attribute. Cannot be any key other than the existing keys in specs . Methods output(): Transform it to the format that the S&O program can read. Returns The file or string that the S&O program can read. convert(specs: ConditionSpec) \u2192 Condition: Parameters specs ( ConditionSpec ): The metric system to be converted to. Returns New Condition instance with the given units. __repr__() \u2192 str: The string representation of the conditions for print() , should be one showing both the values and the corresponding units. Attributes specs ( ConditionSpec ): The specifications of the current design space. Env ( Type ): The Env class that this Condition class is built for. Spec( Type ): The ConditionSpec class dedicated to this Condition class. A mandatory attribute in the definition of Condition subclass. names ( tuple ): The names of the boundary conditions, which determines the order of the values. units ( tuple ): The units of the boundary conditions. Performance class midbench.core.Performance(data: array_like, specs: PerformanceSpec=None, dtype=None, **kwargs) The container and manager for performances. This is the parent class of every other env-specific Performance. It may do two jobs: Storing the performance data produced by simulate() and optimize() . Empty values will be filled with NaN . Indicating the performances to be simulated or optimized (may need to add an additional attribute to PerformanceSpec ). Arguments data ( array_like ): The values of the performances. specs ( PerformanceSpec ): The units of the values. Can be ignored if we only use the default metric system. dtype ( data-type, optional ): dtype for ndarray. kwargs: Optional keyword arguments used as key-value pairs to update the specifications in the specs attribute. Cannot be any key other than the existing keys in specs . Methods output(): Transform it to the format that the S&O program can read. Returns The file or string that the S&O program can read. convert(specs: PerformanceSpec) \u2192 Performance: Parameters specs ( PerformanceSpec ): The units to be converted to. Returns New Performance instance with the given units. __repr__() \u2192 str: The string representation of the performances for print() , should be one showing both the values and the corresponding units. Attributes specs ( PerformanceSpec ): The specifications of the current design space. Env ( Type ): The Env class that this Performance class is built for. Spec( Type ): The PerformanceSpec class dedicated to this Performance class. A mandatory attribute in the definition of Performance subclass. names ( tuple ): The names of the performances, which determines the order of the values. units ( tuple ): The units of the performances. Specifications There might be a tremendous amount of specifications related to each environment components above. To efficiently manage them, enhance the tracability of simulations and optimizations, and improve the compatibility between different subclasses of the environment components, we use the instances of _Spec and _OrderedSpec to record, process and migrate all these specifications. _Spec and _OrderedSpec are dict -like and OrderedDict -like objects, respectively. Their main difference to dictionaries---which is also the reason for the introduction of them---is that they have default keys and values, and their keys are immutable, which makes good sense for specifications. In addition, the specifications is also the defining characteristic of each subclass of the environment components, so it is a necessary attribute of each subclass and it is mandatory to specify it in the definition of each subclass. _Spec This class is for storing the specifications whose order does not matter. For instance, the DesignSpec of Design stores the specifications about the design variable space like the number of control points of the Bezier curve, and we do not care about their order. Subclasses DesignSpec , SimSpec , OptSpec , EnvSpec , MeshSpec ... _OrderedSpec This is a subclass of _Spec for storing specifications whose order matters. For instance, for Condition and Performance , if we want to specify the unit of each dimension of these arrays, we had better also keep this info in a fixed order to remind us of the meaning of each dimension. Subclasses ConditionSpec , PerformanceSpec ... Compatibility There are two ways to create a new _Spec instance, one is to specify explicity what the value of each key is, another is to take an existing _Spec instance as input and copy its values. The latter comes with a problem that two specifications might not have the same set of keys, so that a brainless copy between them is confusing for the fact that the keys of _Spec are fixed. However, this is to our benefit, as every Design , Condition and Performance subclass has a dedicated immutable _Spec which is part of its identity, and we can use this to determine which two subclasses are compatible.","title":"Environments"},{"location":"environments/#mid-benchmark-suite-environments","text":"The environments in MIDBench serve as the interface between Python and the simulation & optimization (S&O) programs---be it a script running locally, or one running inside a Docker or Singularity container. Specifically, it should either perform simulation by taking the design variables and boundary conditions as input then yielding the performance metrics as requested, or perform optimization over the initial design variables by optimizing the performance metrics under the given boundary conditions. Since each S&O program is not necessarily written in Python, the design variables and boundary conditions need to be first transformed into a format readable to the program, and the resulting output of the program needs to be transformed back into Python variables. In short, the MIDBench Environments are for generating the $(\\bm{d}, \\bm{b}, \\bm{p})$---design, condition, performance---tuples for inverse design learning.","title":"MID Benchmark Suite -- Environments"},{"location":"environments/#basic-components","text":"The MIDBench environments should complete their jobs with the following environment-specific Python classes--- Env , Design , Condition , Performance ---as components for easy management:","title":"Basic Components"},{"location":"environments/#env","text":"class midbench.core.Env(specs: EnvSpec = None) The parent class of the simulation/optimization environment, which serves as the interface between Python and the simulation/optimization program. In other words, it takes Python variables as inputs and outputs the results also in form of Python variables, while keeping the processing and running of the program under the hood. Parameters: specs ( EnvSpec ): The specifications for the initialization of the S&O program. Methods As its role suggests, Env should have either a \"simulate\" method, or a \"optimize\" method. Both are for generating the $(\\bm{d}, \\bm{b}, \\bm{p})$ tuples for inverse design learning, but with different motivations. simulate(designs: Design, conditions: Condition, performances: Performance, specs: SimSpec) \u2192 DataEntry: Motivation: Design & Condition \u2192 Performance - Parameters - designs ( Design ) - An $N\\times D$ tensor-liked object defining the design space and storing the set of design variables $\\bm{d}$. - conditions ( Condition ) - An $N\\times B$ or $1\\times B$ tensor-liked object storing the set of boundary conditions $\\bm{b}$. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - performances ( Performance ) - An $N\\times P$ or $1\\times P$ tensor-liked object storing the set of performances. It should be empty, with only the ID of the performances to be evaluated specified. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - specs ( SimSpec ) - The specifications for running the simulation. Should be a Dict -like object. - Returns - The DataEntry data logger storing the $(\\bm{d}, \\bm{b}, \\bm{p})$ tuples, with the SimSpec instance included for simulation replicability. - optimize(designs: Design, conditions: Condition, performances: Performance, specs: OptSpec) \u2192 DataEntry: Motivation: Performance & Condition \u2192 Design - Parameters - designs ( Design ) - An $N\\times D$ tensor-liked object defining the design space and storing the set of initial design variables $\\bm{d}$. - conditions ( Condition ) - An $N\\times B$ or $1\\times B$ tensor-liked object storing the set of boundary conditions $\\bm{b}$ under which the optimization is performed. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - performances ( Performance ) - An $N\\times P$ or $1\\times P$ tensor-liked object storing the set of performances to be optimized. It should be empty, with only the ID of the performances to be evaluated specified. If the first dimension is of size $1$, it will be propagated to all $N$ designs. - specs ( OptSpec ): The specifications for running the optimization, including things like the number of iterations, tolerances, etc. Should be a Dict -like object. - Returns - The DataEntry data entry storing the $(\\bm{d}^\\star, \\bm{b}, \\bm{p}^\\star)$ tuples for either the optimization final result or the entire optimization history, with the OptSpec instance included for optimization replicability. Attributes: dbp (tuple): The tuple (Design, Condition, Performance) of classes dedicated to handling the data for this specific environment.","title":"Env"},{"location":"environments/#design","text":"class midbench.core.Design(data: array_like, specs: DesignSpec=None, dtype=None, **kwargs) An ndarray subclass serving as the container and manager for the design variables $\\bm{d}$ of a list of designs. We can develop environment-specific Designs by inheriting and overloading. Arguments data ( array_like ): The tensor of design variables or the directory of the design files. specs ( DesignSpec ): A dict -like object containing the specifications of the current design variable space. For instance, the specifications of the control point spaces of Bezier curve or B-spline. dtype ( data-type, optional ): dtype for ndarray. kwargs: Optional keyword arguments used as key-value pairs to update the specifications in the specs attribute. Cannot be any key other than the existing keys in specs . Methods convert(specs: DesignSpec) \u2192 Design: Convert the current design variables into a different design variable space. Parameters specs ( DesignSpec ): The specification of the target design variable space. Returns A new Design in the target design space. output(specs: MeshSpec) \u2192 Mesh: Generate mesh file readable to the S&O program. Parameters specs ( MeshSpec ): The specifications of the mesh. Returns A mesh file (or its directory) that is readable to the environment. Attributes specs ( DesignSpec ): The specifications of the current design space. Env ( Type ): The Env class that this Design class is built for. Spec( Type ): The DesignSpec class dedicated to this Design class. A mandatory attribute in the definition of Design subclass.","title":"Design"},{"location":"environments/#condition","text":"class midbench.core.Condition(data: array_like, specs: ConditionSpec=None, dtype=None, **kwargs) The container and manager for boundary conditions. This is the parent class of every other env-specific Condition. Arguments data ( array_like ): The values of the boundary conditions. specs ( ConditionSpec ): An OrderedDict -like object containing the names and units of the values, and also specifying their order. Can be ignored if we only use the default metric system. dtype ( data-type, optional ): dtype for ndarray. kwargs: Optional keyword arguments used as key-value pairs to update the specifications in the specs attribute. Cannot be any key other than the existing keys in specs . Methods output(): Transform it to the format that the S&O program can read. Returns The file or string that the S&O program can read. convert(specs: ConditionSpec) \u2192 Condition: Parameters specs ( ConditionSpec ): The metric system to be converted to. Returns New Condition instance with the given units. __repr__() \u2192 str: The string representation of the conditions for print() , should be one showing both the values and the corresponding units. Attributes specs ( ConditionSpec ): The specifications of the current design space. Env ( Type ): The Env class that this Condition class is built for. Spec( Type ): The ConditionSpec class dedicated to this Condition class. A mandatory attribute in the definition of Condition subclass. names ( tuple ): The names of the boundary conditions, which determines the order of the values. units ( tuple ): The units of the boundary conditions.","title":"Condition"},{"location":"environments/#performance","text":"class midbench.core.Performance(data: array_like, specs: PerformanceSpec=None, dtype=None, **kwargs) The container and manager for performances. This is the parent class of every other env-specific Performance. It may do two jobs: Storing the performance data produced by simulate() and optimize() . Empty values will be filled with NaN . Indicating the performances to be simulated or optimized (may need to add an additional attribute to PerformanceSpec ). Arguments data ( array_like ): The values of the performances. specs ( PerformanceSpec ): The units of the values. Can be ignored if we only use the default metric system. dtype ( data-type, optional ): dtype for ndarray. kwargs: Optional keyword arguments used as key-value pairs to update the specifications in the specs attribute. Cannot be any key other than the existing keys in specs . Methods output(): Transform it to the format that the S&O program can read. Returns The file or string that the S&O program can read. convert(specs: PerformanceSpec) \u2192 Performance: Parameters specs ( PerformanceSpec ): The units to be converted to. Returns New Performance instance with the given units. __repr__() \u2192 str: The string representation of the performances for print() , should be one showing both the values and the corresponding units. Attributes specs ( PerformanceSpec ): The specifications of the current design space. Env ( Type ): The Env class that this Performance class is built for. Spec( Type ): The PerformanceSpec class dedicated to this Performance class. A mandatory attribute in the definition of Performance subclass. names ( tuple ): The names of the performances, which determines the order of the values. units ( tuple ): The units of the performances.","title":"Performance"},{"location":"environments/#specifications","text":"There might be a tremendous amount of specifications related to each environment components above. To efficiently manage them, enhance the tracability of simulations and optimizations, and improve the compatibility between different subclasses of the environment components, we use the instances of _Spec and _OrderedSpec to record, process and migrate all these specifications. _Spec and _OrderedSpec are dict -like and OrderedDict -like objects, respectively. Their main difference to dictionaries---which is also the reason for the introduction of them---is that they have default keys and values, and their keys are immutable, which makes good sense for specifications. In addition, the specifications is also the defining characteristic of each subclass of the environment components, so it is a necessary attribute of each subclass and it is mandatory to specify it in the definition of each subclass.","title":"Specifications"},{"location":"environments/#_spec","text":"This class is for storing the specifications whose order does not matter. For instance, the DesignSpec of Design stores the specifications about the design variable space like the number of control points of the Bezier curve, and we do not care about their order.","title":"_Spec"},{"location":"environments/#subclasses","text":"DesignSpec , SimSpec , OptSpec , EnvSpec , MeshSpec ...","title":"Subclasses"},{"location":"environments/#_orderedspec","text":"This is a subclass of _Spec for storing specifications whose order matters. For instance, for Condition and Performance , if we want to specify the unit of each dimension of these arrays, we had better also keep this info in a fixed order to remind us of the meaning of each dimension.","title":"_OrderedSpec"},{"location":"environments/#subclasses_1","text":"ConditionSpec , PerformanceSpec ...","title":"Subclasses"},{"location":"environments/#compatibility","text":"There are two ways to create a new _Spec instance, one is to specify explicity what the value of each key is, another is to take an existing _Spec instance as input and copy its values. The latter comes with a problem that two specifications might not have the same set of keys, so that a brainless copy between them is confusing for the fact that the keys of _Spec are fixed. However, this is to our benefit, as every Design , Condition and Performance subclass has a dedicated immutable _Spec which is part of its identity, and we can use this to determine which two subclasses are compatible.","title":"Compatibility"},{"location":"installation/","text":"Installing the full MID Benchmark Suite has three main parts: Installing the MID Benchmark Suite core library -- this handles common functions across the entire suite, such as the main classes, computing common ID metrics, etc. Downloading any datasets or pre-trained models that you desire. These are existing curated datasets that we have used in past publications and that are compatible with the MID Benchmark Suite. We do not download these by default, though there are opens in both the install and when using the API to download these when needed. Installing the individual simulation environments -- these are only needed if you wish to run more advanced metrics that require actually evaluating a given engineering solver. You can run certain models and metrics using only the core MID library, however for most meaningful benchmarks you will need to access to at least one simulation environment. Installing the core MID Benchmark Suite Library For the baseline library, you can install this with pip via: pip install midbench Download ID Datasets and Pre-trained Baseline Models Installing Simulation and Optimization Containers Singularity Container Singularity is a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system. Singularity was created to run complex applications on HPC clusters in a simple, portable, and reproducible way. First developed at Lawrence Berkeley National Laboratory, it quickly became popular at other HPC sites, academic sites, and beyond. Singularity is an open-source project, with a friendly community of developers and users. The user base continues to expand, with Singularity now used across industry and academia in many areas of work. Many container platforms are available, but Singularity is focused on: Verifiable reproducibility and security, using cryptographic signatures, an immutable container image format, and in-memory decryption. Integration over isolation by default. Easily make use of GPUs, high speed networks, parallel filesystems on a cluster or server by default. Mobility of compute. The single file SIF container format is easy to transport and share. A simple, effective security model. You are the same user inside a container as outside, and cannot gain additional privilege on the host system by default. Read more about Security in Singularity. For more information about Singularity Container, please check the official website . Installing Singularity You will need a Linux system to run Singularity natively. Options for using Singularity on Mac and Windows machines, along with alternate Linux installation options are discussed here . Install System Dependencies You must first install development libraries to your host. $ sudo apt-get update && sudo apt-get install -y \\ build-essential \\ libssl-dev \\ uuid-dev \\ libgpgme11-dev \\ squashfs-tools \\ libseccomp-dev \\ wget \\ pkg-config \\ git \\ cryptsetup Three steps to install Singularity: Installing GO (Linux) Downloading Singularity Compiling Singularity Source Code Download Singularity from a release You can download Singularity from one of the releases. To see a full list, visit the GitHub release page . After deciding on a release to install, you can run the following commands to proceed with the installation. $ export VERSION=3.5.3 && # adjust this as necessary \\ wget https://github.com/singularityware/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \\ tar -xzf singularity-${VERSION}.tar.gz && \\ cd singularity For our MID Benchmark Suite, the Singularity version 3.5.3 is used to build our Singularity containers for different environments. Users should be able to run the Singularity containers with the same version or higher. Compile the Singularity source code Now you are ready to build Singularity. You can build Singularity using the following commands: $ ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Singularity must be installed as root to function properly. You can verify you've installed Singularity by typing the following command: $ singularity version Confirm that the command prints the installed version of Singularity. We use Singularity containers for many of our different environments. An example of using SU2 Singularity container for 2D airfoil simulation and optimization can be found in one of our tutorials about the 2D airfoil inverse design . Docker Container Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Docker provides tooling and a platform to manage the lifecycle of your containers: Develop your application and its supporting components using containers. The container becomes the unit for distributing and testing your application. When you\u2019re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two. For more information about Docker, please check the official website . Installing Docker Linux Users: Please use the following commands to install Docker: $ sudo apt-get remove docker docker-engine docker.io && \\ apt-get update && \\ apt install docker.io Windows and Mac Users: Mac and Windows users should install the Docker Toolbox (this is a simple one-click install). If running on Mac or Windows, make sure you run the following commands inside the Docker Quickstart Terminal. We use Docker container for our 2D heat exchanger optimization environment. Please find the usage of the dolfin-adjoint Docker in this tutorial .","title":"Installation"},{"location":"installation/#installing-the-core-mid-benchmark-suite-library","text":"For the baseline library, you can install this with pip via: pip install midbench","title":"Installing the core MID Benchmark Suite Library"},{"location":"installation/#download-id-datasets-and-pre-trained-baseline-models","text":"","title":"Download ID Datasets and Pre-trained Baseline Models"},{"location":"installation/#installing-simulation-and-optimization-containers","text":"","title":"Installing Simulation and Optimization Containers"},{"location":"installation/#singularity-container","text":"Singularity is a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system. Singularity was created to run complex applications on HPC clusters in a simple, portable, and reproducible way. First developed at Lawrence Berkeley National Laboratory, it quickly became popular at other HPC sites, academic sites, and beyond. Singularity is an open-source project, with a friendly community of developers and users. The user base continues to expand, with Singularity now used across industry and academia in many areas of work. Many container platforms are available, but Singularity is focused on: Verifiable reproducibility and security, using cryptographic signatures, an immutable container image format, and in-memory decryption. Integration over isolation by default. Easily make use of GPUs, high speed networks, parallel filesystems on a cluster or server by default. Mobility of compute. The single file SIF container format is easy to transport and share. A simple, effective security model. You are the same user inside a container as outside, and cannot gain additional privilege on the host system by default. Read more about Security in Singularity. For more information about Singularity Container, please check the official website .","title":"Singularity Container"},{"location":"installation/#installing-singularity","text":"You will need a Linux system to run Singularity natively. Options for using Singularity on Mac and Windows machines, along with alternate Linux installation options are discussed here .","title":"Installing Singularity"},{"location":"installation/#install-system-dependencies","text":"You must first install development libraries to your host. $ sudo apt-get update && sudo apt-get install -y \\ build-essential \\ libssl-dev \\ uuid-dev \\ libgpgme11-dev \\ squashfs-tools \\ libseccomp-dev \\ wget \\ pkg-config \\ git \\ cryptsetup","title":"Install System Dependencies"},{"location":"installation/#three-steps-to-install-singularity","text":"Installing GO (Linux) Downloading Singularity Compiling Singularity Source Code","title":"Three steps to install Singularity:"},{"location":"installation/#download-singularity-from-a-release","text":"You can download Singularity from one of the releases. To see a full list, visit the GitHub release page . After deciding on a release to install, you can run the following commands to proceed with the installation. $ export VERSION=3.5.3 && # adjust this as necessary \\ wget https://github.com/singularityware/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \\ tar -xzf singularity-${VERSION}.tar.gz && \\ cd singularity For our MID Benchmark Suite, the Singularity version 3.5.3 is used to build our Singularity containers for different environments. Users should be able to run the Singularity containers with the same version or higher.","title":"Download Singularity from a release"},{"location":"installation/#compile-the-singularity-source-code","text":"Now you are ready to build Singularity. You can build Singularity using the following commands: $ ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Singularity must be installed as root to function properly. You can verify you've installed Singularity by typing the following command: $ singularity version Confirm that the command prints the installed version of Singularity. We use Singularity containers for many of our different environments. An example of using SU2 Singularity container for 2D airfoil simulation and optimization can be found in one of our tutorials about the 2D airfoil inverse design .","title":"Compile the Singularity source code"},{"location":"installation/#docker-container","text":"Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Docker provides tooling and a platform to manage the lifecycle of your containers: Develop your application and its supporting components using containers. The container becomes the unit for distributing and testing your application. When you\u2019re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two. For more information about Docker, please check the official website .","title":"Docker Container"},{"location":"installation/#installing-docker","text":"Linux Users: Please use the following commands to install Docker: $ sudo apt-get remove docker docker-engine docker.io && \\ apt-get update && \\ apt install docker.io Windows and Mac Users: Mac and Windows users should install the Docker Toolbox (this is a simple one-click install). If running on Mac or Windows, make sure you run the following commands inside the Docker Quickstart Terminal. We use Docker container for our 2D heat exchanger optimization environment. Please find the usage of the dolfin-adjoint Docker in this tutorial .","title":"Installing Docker"},{"location":"metrics/","text":"MID Benchmark Suite -- Metrics The metrics are functions for evaluating the performance of the inverse design models and the quality of the dataset. Because of the difference between their roles and their underlying mechanisms, they are divided into three different categories in MID. Internal Distribution Metrics This set of metrics could evaluate the performance of inverse design model purely using the existing dataset by relying on statistics, without running the MIDBench environments to do time-consuming simulation or optimization. def midbench.metrics.maximum_mean_discrepancy(gen_func: Function, X_test: Ndarray) For evaluating the maximum mean discrepancy between the inverse design generator distribution and the dataset empirical distribution. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. Returns The score. def midbench.metrics.consistency(gen_func, latent_dim, bounds) For evaluating the latent space consistency of the generator function inside the given latent space. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. latent_dim (int): The dataset in the form of numpy array. bounds (list) Returns The score. def midbench.metrics.rsmth(gen_func, X_test, N=2000) For evaluating the generator function's relative variance of difference against the dataset. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. N (int): The number of samples to be generated for the evaluation. Returns The score. def midbench.metrics.rdiv(gen_func, X_train, d=None, k=None, bounds=None) For evaluating the generator function's relative diversity against the dataset. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. Returns The score. def midbench.metrics.mean_log_likelihood(gen_func, X_test, N=2000) This metric first fits the generator's samples with a kernel density estimation function, then take the dataset's log likelihood on this KDE model as the score. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. N (int): The number of samples to be generated for the evaluation. Returns The score. External Performance Metrics This set of metrics directly evaluates the quality of inverse design model with the MIDBench environment by computing quantities like the Cumulative Optimality Gap and the Instantaneous Optimality Gap. These are currently not yet implemented in the public library but we will update here when these are available. Dataset Metrics This set of metrics for evaluating the quality of the dataset or generated samples, such as the diversity of it. Can be used for Database curation. Utils def confidence_interval(metric: Function) \u2192 Function A decorator function for turning any non-deterministic metric into one evaluating the confidence interval instead. Parameters metric ( Function ): A metric function with certain input. Returns ci_metric ( Function ): A function that accepts an additional ci_n as the first positional argument for the number of sampling batches. The subsequent input arguments are the same as metric 's.","title":"Metrics"},{"location":"metrics/#mid-benchmark-suite-metrics","text":"The metrics are functions for evaluating the performance of the inverse design models and the quality of the dataset. Because of the difference between their roles and their underlying mechanisms, they are divided into three different categories in MID.","title":"MID Benchmark Suite -- Metrics"},{"location":"metrics/#internal-distribution-metrics","text":"This set of metrics could evaluate the performance of inverse design model purely using the existing dataset by relying on statistics, without running the MIDBench environments to do time-consuming simulation or optimization. def midbench.metrics.maximum_mean_discrepancy(gen_func: Function, X_test: Ndarray) For evaluating the maximum mean discrepancy between the inverse design generator distribution and the dataset empirical distribution. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. Returns The score. def midbench.metrics.consistency(gen_func, latent_dim, bounds) For evaluating the latent space consistency of the generator function inside the given latent space. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. latent_dim (int): The dataset in the form of numpy array. bounds (list) Returns The score. def midbench.metrics.rsmth(gen_func, X_test, N=2000) For evaluating the generator function's relative variance of difference against the dataset. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. N (int): The number of samples to be generated for the evaluation. Returns The score. def midbench.metrics.rdiv(gen_func, X_train, d=None, k=None, bounds=None) For evaluating the generator function's relative diversity against the dataset. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. Returns The score. def midbench.metrics.mean_log_likelihood(gen_func, X_test, N=2000) This metric first fits the generator's samples with a kernel density estimation function, then take the dataset's log likelihood on this KDE model as the score. Parameters gen_func (Function): The generator function which produces certain amount of samples when called. X_test (Ndarray): The dataset in the form of numpy array. N (int): The number of samples to be generated for the evaluation. Returns The score.","title":"Internal Distribution Metrics"},{"location":"metrics/#external-performance-metrics","text":"This set of metrics directly evaluates the quality of inverse design model with the MIDBench environment by computing quantities like the Cumulative Optimality Gap and the Instantaneous Optimality Gap. These are currently not yet implemented in the public library but we will update here when these are available.","title":"External Performance Metrics"},{"location":"metrics/#dataset-metrics","text":"This set of metrics for evaluating the quality of the dataset or generated samples, such as the diversity of it. Can be used for Database curation.","title":"Dataset Metrics"},{"location":"metrics/#utils","text":"def confidence_interval(metric: Function) \u2192 Function A decorator function for turning any non-deterministic metric into one evaluating the confidence interval instead. Parameters metric ( Function ): A metric function with certain input. Returns ci_metric ( Function ): A function that accepts an additional ci_n as the first positional argument for the number of sampling batches. The subsequent input arguments are the same as metric 's.","title":"Utils"},{"location":"tutorials/","text":"Tutorials This page lists various tutorials for using the benchmark suite.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"This page lists various tutorials for using the benchmark suite.","title":"Tutorials"},{"location":"environments/airfoil2d-su2/","text":"airfoil2d-su2 Environment Documentation The environment wraps the SU2 CFD solver, which we use for 2D and 3D aero and aero-structural examples. For now, the existing 2D airfoil tutorial provides the best overview for installing that specific example. As we add more examples, we will provide common installation instructions here.","title":"airfoil2d-su2"},{"location":"environments/airfoil2d-su2/#airfoil2d-su2-environment-documentation","text":"The environment wraps the SU2 CFD solver, which we use for 2D and 3D aero and aero-structural examples. For now, the existing 2D airfoil tutorial provides the best overview for installing that specific example. As we add more examples, we will provide common installation instructions here.","title":"airfoil2d-su2 Environment Documentation"},{"location":"environments/airfoil3d-su2/","text":"airfoil3d-su2 Environment Documentation","title":"Airfoil3d su2"},{"location":"environments/airfoil3d-su2/#airfoil3d-su2-environment-documentation","text":"","title":"airfoil3d-su2 Environment Documentation"},{"location":"environments/heatsink2d-dolfin-adjoint/","text":"heatsink2d-dolfin-adjoint Environment Documentation The environment wraps the FEniCS and dolfin-adjoint multiphysics solvers, which we use for a variety of applications and problems. For now, our only publically available example is the 2D Heat Conduction tutorial which describes how to install that specific example. As we add more examples, we will provide common installation instructions here.","title":"heatsink2d-dolfin-adjoint"},{"location":"environments/heatsink2d-dolfin-adjoint/#heatsink2d-dolfin-adjoint-environment-documentation","text":"The environment wraps the FEniCS and dolfin-adjoint multiphysics solvers, which we use for a variety of applications and problems. For now, our only publically available example is the 2D Heat Conduction tutorial which describes how to install that specific example. As we add more examples, we will provide common installation instructions here.","title":"heatsink2d-dolfin-adjoint Environment Documentation"},{"location":"solvers/dolfin-adjoint/","text":"Dolfin-Adjoint Documentation","title":"Dolfin adjoint"},{"location":"solvers/dolfin-adjoint/#dolfin-adjoint-documentation","text":"","title":"Dolfin-Adjoint Documentation"},{"location":"solvers/mto-openfoam/","text":"MTO/OpenFoam Solver Documentation","title":"Mto openfoam"},{"location":"solvers/mto-openfoam/#mtoopenfoam-solver-documentation","text":"","title":"MTO/OpenFoam Solver Documentation"},{"location":"solvers/ngSpice/","text":"ngSpice Solver Documentation","title":"ngSpice"},{"location":"solvers/ngSpice/#ngspice-solver-documentation","text":"","title":"ngSpice Solver Documentation"},{"location":"solvers/su2/","text":"SU2 Solver Documentation","title":"Su2"},{"location":"solvers/su2/#su2-solver-documentation","text":"","title":"SU2 Solver Documentation"},{"location":"tutorials/airfoil-su2-singularity/","text":"Usage SU2 Installation Users who are interested in the open-source SU2 CFD solver can install the SU2 suite directly on you devices by following the intallation guide on SU2 official website. Linux and Mac Users: Please find the installation steps here to install SU2 and enable SU2 python library: Download source code: Please first download the source code from the SU2 download webpage . Create a configuration using the meson.py : ./meson.py build -Denable-autodiff=true . Set environment variables: There will be an instruction showing up in the terminal window for setting environment variables after creating the configuration. Users can copy and paste the paths to the ~/.bashrc and source ~/.bashrc . If you miss the instruction, please follow this link to set up the environment variables. Compile and install SU2: ./ninja -C build install . Windows Users: Please install SU2 and enable SU2 python library using the steps shown here . SU2 Singularity Container (Recommended) We perform the 2D airfoil simulation and optimization using SU2 CFD solver. For user's easy usage, we have pre-built the SU2 suite using a singularity container with the required environments set up. The ready-to-use SU2 singularity container can be pulled or downloaded from https://cloud.sylabs.io/library/junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh . To run the MIDBench API code : Linux Users: If you are using Linux (e.g., Ubuntu) or Linux VM box), please do the following steps: Install Singularity. If you already have singularity installed on your device, please ignore this step. If not, please follow the 3-step quick installation guide to install Singularity first. Our MID Benchmark Suite uses Singularity version 3.5.3. You should be able to run the SIF container with the same version or higher. Download SU2 SIF container. Please download the SU2 SIF container from Sylabs using this link ( https://cloud.sylabs.io/library/junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh ). After installing Singularity, you can also pull the SU2 SIF container using the following command: $ singularity pull --arch amd64 library://junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh:latest Enter SU2 SIF container by typing the following command in your terminal: singularity shell /path/to/your/downloaded/su2v7.3.1_conda3.9.12_gmsh_latest.sif Open jupyter notebook or jupyter lab by typing command in your terminal: jupyter lab or jupyter-notebook . In the tutorial directory, there is an iPython notebook \"example_usage.ipynb\" . Please open the notebook in jupyter lab or jupyter notebook. Please run the API demo code in the notebook cell by cell to check out the 2D Airfoil Simulation and Optimization demos in your SU2 SIF container. Windows Users: If you are using Windows system and not interested in installing the Linux system or subsystem, you can directly implement our MIDBench API demos in Google Colab Notebook by following the installation and implementation steps below (we also include an iPython notebook \"singularity_SU2.ipynb\" for Windows users to use directly in Colab): Install Singularity: Please follow the 3-step quick installation guide to install Singularity. Pull SU2 SIF Container: !singularity pull --arch amd64 library://junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh:latest Make sure you have installed midbench: pip install midbench Create and Execute Python Scripts using SIF Container (recommended): # Simulation %%writefile MIDBenchmarkSuite/airfoil2d_simu.py from midbench.envs import make Env, Design, Condition = make(\"Airfoil2d-v0\") designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy').meshgen() conditions = Condition(**{'mach':0.7,'reynolds':7000000,'lift':0.350}) performances = ['lift', 'drag'] lift, drag = Env.simulate(conditions, designs, performances, './midbench/envs/airfoil/results_simu') print(lift, drag) # Simulation %%bash cd MIDBenchmarkSuite/ singularity exec --userns ../su2v7.3.1_conda3.9.12_gmsh_latest.sif bash -c \"python airfoil2d_simu.py\" # Optimization %%writefile MIDBenchmarkSuite/airfoil2d_opt.py from midbench.envs import make Env, Design, Condition = make(\"Airfoil2d-v0\") designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy') conditions = Condition(**{'mach':0.6,'reynolds':8000000,'lift':0.320}) objectives = ['drag', 'ld_ratio', 'airfoil_opt'] drag, ld_ratio, airfoil_opt = Env.optimize(conditions, designs, objectives, './midbench/envs/airfoil/results_opt') print(drag, ld_ratio, airfoil_opt) # Optimization %%bash cd MIDBenchmarkSuite/ singularity exec --userns ../su2v7.3.1_conda3.9.12_gmsh_latest.sif bash -c \"python airfoil2d_opt.py\" Alternatively, You can Spawn a New Shell within SIF Container: singularity shell --userns su2v7.3.1_conda3.9.12_gmsh_latest.sif After entering SU2 SIF container, please run the 2D airfoil simulation and optimization demos using the following commands: Type python . In the Python environment, please enter the following commands to run demos: from midbench.envs import make Env, Design, Condition = make(\"Airfoil2d-v0\") # Simulation designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy').meshgen() conditions = Condition(**{'mach':0.7,'reynolds':7000000,'lift':0.350}) performances = ['lift', 'drag'] lift, drag = Env.simulate(conditions, designs, performances, './midbench/envs/airfoil/results_simu') # Optimization designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy') conditions = Condition(**{'mach':0.6,'reynolds':8000000,'lift':0.320}) objectives = ['drag', 'ld_ratio', 'airfoil_opt'] drag, ld_ratio, airfoil_opt = Env.optimize(conditions, designs, objectives, './midbench/envs/airfoil/results_opt') print(drag, ld_ratio, airfoil_opt) NOTE : The singuarity container also includes a 2D airfoil mesh generator AirfoilGeometryConverter . The generator will first convert the 2D coordinates of the airfoil curve points into mesh. The SU2 solver then takes the mesh for the follwoing CFD simulation and shape optimization.","title":"Conditional GAN Models for 2D Airfoil Optimization"},{"location":"tutorials/airfoil-su2-singularity/#usage","text":"","title":"Usage"},{"location":"tutorials/airfoil-su2-singularity/#su2-installation","text":"Users who are interested in the open-source SU2 CFD solver can install the SU2 suite directly on you devices by following the intallation guide on SU2 official website. Linux and Mac Users: Please find the installation steps here to install SU2 and enable SU2 python library: Download source code: Please first download the source code from the SU2 download webpage . Create a configuration using the meson.py : ./meson.py build -Denable-autodiff=true . Set environment variables: There will be an instruction showing up in the terminal window for setting environment variables after creating the configuration. Users can copy and paste the paths to the ~/.bashrc and source ~/.bashrc . If you miss the instruction, please follow this link to set up the environment variables. Compile and install SU2: ./ninja -C build install . Windows Users: Please install SU2 and enable SU2 python library using the steps shown here .","title":"SU2 Installation"},{"location":"tutorials/airfoil-su2-singularity/#su2-singularity-container-recommended","text":"We perform the 2D airfoil simulation and optimization using SU2 CFD solver. For user's easy usage, we have pre-built the SU2 suite using a singularity container with the required environments set up. The ready-to-use SU2 singularity container can be pulled or downloaded from https://cloud.sylabs.io/library/junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh . To run the MIDBench API code : Linux Users: If you are using Linux (e.g., Ubuntu) or Linux VM box), please do the following steps: Install Singularity. If you already have singularity installed on your device, please ignore this step. If not, please follow the 3-step quick installation guide to install Singularity first. Our MID Benchmark Suite uses Singularity version 3.5.3. You should be able to run the SIF container with the same version or higher. Download SU2 SIF container. Please download the SU2 SIF container from Sylabs using this link ( https://cloud.sylabs.io/library/junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh ). After installing Singularity, you can also pull the SU2 SIF container using the following command: $ singularity pull --arch amd64 library://junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh:latest Enter SU2 SIF container by typing the following command in your terminal: singularity shell /path/to/your/downloaded/su2v7.3.1_conda3.9.12_gmsh_latest.sif Open jupyter notebook or jupyter lab by typing command in your terminal: jupyter lab or jupyter-notebook . In the tutorial directory, there is an iPython notebook \"example_usage.ipynb\" . Please open the notebook in jupyter lab or jupyter notebook. Please run the API demo code in the notebook cell by cell to check out the 2D Airfoil Simulation and Optimization demos in your SU2 SIF container. Windows Users: If you are using Windows system and not interested in installing the Linux system or subsystem, you can directly implement our MIDBench API demos in Google Colab Notebook by following the installation and implementation steps below (we also include an iPython notebook \"singularity_SU2.ipynb\" for Windows users to use directly in Colab): Install Singularity: Please follow the 3-step quick installation guide to install Singularity. Pull SU2 SIF Container: !singularity pull --arch amd64 library://junideallab/midbench/su2v7.3.1_conda3.9.12_gmsh:latest Make sure you have installed midbench: pip install midbench Create and Execute Python Scripts using SIF Container (recommended): # Simulation %%writefile MIDBenchmarkSuite/airfoil2d_simu.py from midbench.envs import make Env, Design, Condition = make(\"Airfoil2d-v0\") designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy').meshgen() conditions = Condition(**{'mach':0.7,'reynolds':7000000,'lift':0.350}) performances = ['lift', 'drag'] lift, drag = Env.simulate(conditions, designs, performances, './midbench/envs/airfoil/results_simu') print(lift, drag) # Simulation %%bash cd MIDBenchmarkSuite/ singularity exec --userns ../su2v7.3.1_conda3.9.12_gmsh_latest.sif bash -c \"python airfoil2d_simu.py\" # Optimization %%writefile MIDBenchmarkSuite/airfoil2d_opt.py from midbench.envs import make Env, Design, Condition = make(\"Airfoil2d-v0\") designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy') conditions = Condition(**{'mach':0.6,'reynolds':8000000,'lift':0.320}) objectives = ['drag', 'ld_ratio', 'airfoil_opt'] drag, ld_ratio, airfoil_opt = Env.optimize(conditions, designs, objectives, './midbench/envs/airfoil/results_opt') print(drag, ld_ratio, airfoil_opt) # Optimization %%bash cd MIDBenchmarkSuite/ singularity exec --userns ../su2v7.3.1_conda3.9.12_gmsh_latest.sif bash -c \"python airfoil2d_opt.py\" Alternatively, You can Spawn a New Shell within SIF Container: singularity shell --userns su2v7.3.1_conda3.9.12_gmsh_latest.sif After entering SU2 SIF container, please run the 2D airfoil simulation and optimization demos using the following commands: Type python . In the Python environment, please enter the following commands to run demos: from midbench.envs import make Env, Design, Condition = make(\"Airfoil2d-v0\") # Simulation designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy').meshgen() conditions = Condition(**{'mach':0.7,'reynolds':7000000,'lift':0.350}) performances = ['lift', 'drag'] lift, drag = Env.simulate(conditions, designs, performances, './midbench/envs/airfoil/results_simu') # Optimization designs = Design('./midbench/envs/airfoil/airfoils_pred_cbegan_example.npy') conditions = Condition(**{'mach':0.6,'reynolds':8000000,'lift':0.320}) objectives = ['drag', 'ld_ratio', 'airfoil_opt'] drag, ld_ratio, airfoil_opt = Env.optimize(conditions, designs, objectives, './midbench/envs/airfoil/results_opt') print(drag, ld_ratio, airfoil_opt) NOTE : The singuarity container also includes a 2D airfoil mesh generator AirfoilGeometryConverter . The generator will first convert the 2D coordinates of the airfoil curve points into mesh. The SU2 solver then takes the mesh for the follwoing CFD simulation and shape optimization.","title":"SU2 Singularity Container (Recommended)"},{"location":"tutorials/heat-fenics-docker/","text":"Usage dolfin-adjoint with FEniCS Docker To Run the MIDBench API Code: Install midbench: pip install midbench Create a session that has access to the current folder from the host: Linux Users: $ cd /path/to/midbench $ sudo docker run -it -p 8887:8887 -v $(pwd):/home/fenics/shared quay.io/dolfinadjoint/pyadjoint Windows and Mac Users: If running on Mac or Windows, make sure you run the above commands inside the Docker Quickstart Terminal. Open jupyter notebook or jupyter lab: $ jupyter lab --no-browser --ip=0.0.0.0 --port=8887 Run MIDBench 2D Heat Conduction Optimization Demo: In the midbench directory, there is an iPython notebook \"example_usage.ipynb\". Please open the notebook in jupyter lab or jupyter notebook. Please run the API demo code in the notebook cell by cell to check out the 2D Heat Conduction Optimization demo in your dolfin-adjoint FEniCS Docker container. The commands in the jupyter notebook are as follows: # Optimization from midbench.envs import make Env, Design, Condition = make(\"HeatConduction2d-v0\") conditions = Condition(**{'volume':0.4,'length':0.5,'resolution':50}) designs = Design(**{'volume':0.4,'resolution':50}).output() objectives = ['compliance'] compliance=Env.optimize(conditions, designs, objectives)","title":"2D Heat Conduction Optimization"},{"location":"tutorials/heat-fenics-docker/#usage","text":"","title":"Usage"},{"location":"tutorials/heat-fenics-docker/#dolfin-adjoint-with-fenics-docker","text":"","title":"dolfin-adjoint with FEniCS Docker"},{"location":"tutorials/heat-fenics-docker/#to-run-the-midbench-api-code","text":"Install midbench: pip install midbench Create a session that has access to the current folder from the host: Linux Users: $ cd /path/to/midbench $ sudo docker run -it -p 8887:8887 -v $(pwd):/home/fenics/shared quay.io/dolfinadjoint/pyadjoint Windows and Mac Users: If running on Mac or Windows, make sure you run the above commands inside the Docker Quickstart Terminal. Open jupyter notebook or jupyter lab: $ jupyter lab --no-browser --ip=0.0.0.0 --port=8887 Run MIDBench 2D Heat Conduction Optimization Demo: In the midbench directory, there is an iPython notebook \"example_usage.ipynb\". Please open the notebook in jupyter lab or jupyter notebook. Please run the API demo code in the notebook cell by cell to check out the 2D Heat Conduction Optimization demo in your dolfin-adjoint FEniCS Docker container. The commands in the jupyter notebook are as follows: # Optimization from midbench.envs import make Env, Design, Condition = make(\"HeatConduction2d-v0\") conditions = Condition(**{'volume':0.4,'length':0.5,'resolution':50}) designs = Design(**{'volume':0.4,'resolution':50}).output() objectives = ['compliance'] compliance=Env.optimize(conditions, designs, objectives)","title":"To Run the MIDBench API Code:"}]}